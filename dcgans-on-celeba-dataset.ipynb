{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dset\nimport torchvision.utils as vutils\nimport os\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:28:15.825566Z","iopub.execute_input":"2025-03-07T12:28:15.825901Z","iopub.status.idle":"2025-03-07T12:28:15.830530Z","shell.execute_reply.started":"2025-03-07T12:28:15.825873Z","shell.execute_reply":"2025-03-07T12:28:15.829416Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class CelebADataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset class for loading images from the CelebA dataset.\n\n    This class loads images from a specified directory without requiring class subfolders,\n    making it suitable for datasets where images are stored in a single directory.\n\n    Attributes:\n        root_dir (str): Path to the directory containing CelebA images.\n        transform (callable, optional): A function/transform to apply to the images.\n\n    Methods:\n        __len__(): Returns the number of images in the dataset.\n        __getitem__(idx): Loads an image at the given index, applies transformations (if any),\n                          and returns the transformed image along with a dummy label (0).\n\n    \"\"\"\n\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Initializes the dataset by collecting all image file paths.\n        Args:\n            root_dir (str): Directory where CelebA images are stored.\n            transform (callable, optional): Transformations to apply to images (default is None).\n        \"\"\"\n        self.files = sorted(glob.glob(os.path.join(root_dir, \"*.jpg\"))) \n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of images in the dataset.\n        Returns:\n            int: Number of images.\n        \"\"\"\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Loads an image from the dataset, applies transformations, and returns it.\n        Args:\n            idx (int): Index of the image to load.\n        Returns:\n            Tuple[Tensor, int]: Transformed image as a PyTorch tensor and a dummy label (0).\n        \"\"\"\n        img_path = self.files[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, 0  # No labels in CelebA, so return a dummy label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:27:35.781935Z","iopub.execute_input":"2025-03-07T12:27:35.782209Z","iopub.status.idle":"2025-03-07T12:27:35.787154Z","shell.execute_reply.started":"2025-03-07T12:27:35.782188Z","shell.execute_reply":"2025-03-07T12:27:35.786198Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# transformations include resizing to 64x64, center cropping and normalization of gray values t0 [0, 1]\ntransform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.CenterCrop(64),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:27:59.463909Z","iopub.execute_input":"2025-03-07T12:27:59.464186Z","iopub.status.idle":"2025-03-07T12:27:59.468191Z","shell.execute_reply.started":"2025-03-07T12:27:59.464164Z","shell.execute_reply":"2025-03-07T12:27:59.467364Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# specifying dataset path and creating custom dataset class object\nDATA_PATH = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/\"\ndataset = CelebADataset(root_dir=DATA_PATH, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:28:18.211931Z","iopub.execute_input":"2025-03-07T12:28:18.212233Z","iopub.status.idle":"2025-03-07T12:28:19.692368Z","shell.execute_reply.started":"2025-03-07T12:28:18.212207Z","shell.execute_reply":"2025-03-07T12:28:19.691553Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# creating a dataloader object for CelebA dataset\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:28:34.644836Z","iopub.execute_input":"2025-03-07T12:28:34.645149Z","iopub.status.idle":"2025-03-07T12:28:34.649177Z","shell.execute_reply.started":"2025-03-07T12:28:34.645121Z","shell.execute_reply":"2025-03-07T12:28:34.648262Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# initialize the variables\nLATENT_DIM = 100\nEPOCHS = 10\nLEARNING_RATE = 0.0002\nBETA1 = 0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:34.451048Z","iopub.execute_input":"2025-03-07T12:35:34.451333Z","iopub.status.idle":"2025-03-07T12:35:34.455528Z","shell.execute_reply.started":"2025-03-07T12:35:34.451310Z","shell.execute_reply":"2025-03-07T12:35:34.454793Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class Generator(nn.Module):\n    \"\"\"\n    DCGAN Generator model that transforms a latent noise vector into a 64x64 RGB image.\n    Architecture:\n    - Uses transposed convolutions for upsampling.\n    - Batch normalization and ReLU activations in all layers except the output.\n    - Final layer uses Tanh activation to scale pixel values to [-1, 1].\n    Args:\n        LATENT_DIM (int): Size of the input noise vector.\n    Forward Pass:\n        x (Tensor): Input noise tensor of shape (batch_size, LATENT_DIM, 1, 1).\n        Returns: Generated image tensor of shape (batch_size, 3, 64, 64).\n    \"\"\"\n\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(LATENT_DIM, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:35.181843Z","iopub.execute_input":"2025-03-07T12:35:35.182126Z","iopub.status.idle":"2025-03-07T12:35:35.187604Z","shell.execute_reply.started":"2025-03-07T12:35:35.182104Z","shell.execute_reply":"2025-03-07T12:35:35.186771Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \"\"\"\n    DCGAN Discriminator model that classifies images as real or fake.\n    Architecture:\n    - Uses strided convolutions for downsampling.\n    - Batch normalization after the first layer for stable training.\n    - LeakyReLU activation (slope=0.2) for all layers except the output.\n    - Final layer uses a Sigmoid activation to output a probability.\n    Forward Pass:\n        x (Tensor): Input image tensor of shape (batch_size, 3, 64, 64).\n        Returns: Probability tensor of shape (batch_size, 1) indicating real (1) or fake (0).\n    \"\"\"\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:35.690580Z","iopub.execute_input":"2025-03-07T12:35:35.690927Z","iopub.status.idle":"2025-03-07T12:35:35.698741Z","shell.execute_reply.started":"2025-03-07T12:35:35.690901Z","shell.execute_reply":"2025-03-07T12:35:35.697627Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def weights_init(m):\n    \"\"\"\n    Initializes the weights of convolutional and batch normalization layers \n    in the Generator and Discriminator models as per the DCGAN paper.\n\n    Initialization Details:\n    - Convolutional layers: Weights are initialized from a normal distribution \n      with mean 0 and standard deviation 0.02.\n    - BatchNorm layers: Weights are initialized from a normal distribution \n      (mean=1, std=0.02), and biases are set to 0.\n\n    Args:\n        m (nn.Module): A model layer passed during model initialization.\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:36.180007Z","iopub.execute_input":"2025-03-07T12:35:36.180325Z","iopub.status.idle":"2025-03-07T12:35:36.184831Z","shell.execute_reply.started":"2025-03-07T12:35:36.180297Z","shell.execute_reply":"2025-03-07T12:35:36.184090Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# instantiating the models\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:36.794688Z","iopub.execute_input":"2025-03-07T12:35:36.794966Z","iopub.status.idle":"2025-03-07T12:35:36.847928Z","shell.execute_reply.started":"2025-03-07T12:35:36.794944Z","shell.execute_reply":"2025-03-07T12:35:36.847257Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# initialize the model weights\ngenerator.apply(weights_init)\ndiscriminator.apply(weights_init)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:38.087156Z","iopub.execute_input":"2025-03-07T12:35:38.087600Z","iopub.status.idle":"2025-03-07T12:35:38.094141Z","shell.execute_reply.started":"2025-03-07T12:35:38.087555Z","shell.execute_reply":"2025-03-07T12:35:38.093435Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Discriminator(\n  (model): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# specify loss criteria and optimizers for generator and discriminator\ncriterion = nn.BCELoss()\noptimizerG = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:39.797792Z","iopub.execute_input":"2025-03-07T12:35:39.798070Z","iopub.status.idle":"2025-03-07T12:35:39.803105Z","shell.execute_reply.started":"2025-03-07T12:35:39.798048Z","shell.execute_reply":"2025-03-07T12:35:39.802353Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# train the model and save generated images\nfor epoch in range(EPOCHS):\n    for i, (real_images, _) in enumerate(dataloader):\n        discriminator.zero_grad()\n        real_images = real_images.to(device)\n        real_labels = torch.ones(real_images.size(0), 1, device=device)\n        fake_labels = torch.zeros(real_images.size(0), 1, device=device)\n        \n        output = discriminator(real_images).view(-1, 1)\n        loss_real = criterion(output, real_labels)\n        loss_real.backward()\n        \n        noise = torch.randn(real_images.size(0), LATENT_DIM, 1, 1, device=device)\n        fake_images = generator(noise)\n        output = discriminator(fake_images.detach()).view(-1, 1)\n        loss_fake = criterion(output, fake_labels)\n        loss_fake.backward()\n        optimizerD.step()\n        \n        generator.zero_grad()\n        output = discriminator(fake_images).view(-1, 1)\n        loss_G = criterion(output, real_labels)\n        loss_G.backward()\n        optimizerG.step()\n        \n        if i % 100 == 0:\n            print(f\"Epoch [{epoch}/{EPOCHS}] Step [{i}/{len(dataloader)}] Loss D: {loss_real + loss_fake:.4f}, Loss G: {loss_G:.4f}\")\n    \n    vutils.save_image(fake_images, f\"fake_images_epoch_{epoch}.png\", normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:41.981168Z","iopub.execute_input":"2025-03-07T12:35:41.981454Z","iopub.status.idle":"2025-03-07T14:07:01.001692Z","shell.execute_reply.started":"2025-03-07T12:35:41.981432Z","shell.execute_reply":"2025-03-07T14:07:01.000974Z"}},"outputs":[{"name":"stdout","text":"Epoch [0/10] Step [0/1583] Loss D: 1.5798, Loss G: 3.2713\nEpoch [0/10] Step [100/1583] Loss D: 0.7159, Loss G: 7.9165\nEpoch [0/10] Step [200/1583] Loss D: 0.3347, Loss G: 5.9586\nEpoch [0/10] Step [300/1583] Loss D: 0.5652, Loss G: 3.1923\nEpoch [0/10] Step [400/1583] Loss D: 0.8153, Loss G: 6.9473\nEpoch [0/10] Step [500/1583] Loss D: 0.5408, Loss G: 3.5140\nEpoch [0/10] Step [600/1583] Loss D: 0.4716, Loss G: 3.0437\nEpoch [0/10] Step [700/1583] Loss D: 0.7238, Loss G: 3.4431\nEpoch [0/10] Step [800/1583] Loss D: 0.7049, Loss G: 6.8274\nEpoch [0/10] Step [900/1583] Loss D: 0.6737, Loss G: 6.0510\nEpoch [0/10] Step [1000/1583] Loss D: 0.7181, Loss G: 6.7699\nEpoch [0/10] Step [1100/1583] Loss D: 0.4998, Loss G: 5.3280\nEpoch [0/10] Step [1200/1583] Loss D: 0.2871, Loss G: 5.0793\nEpoch [0/10] Step [1300/1583] Loss D: 0.5343, Loss G: 5.6479\nEpoch [0/10] Step [1400/1583] Loss D: 0.4275, Loss G: 3.4961\nEpoch [0/10] Step [1500/1583] Loss D: 0.9757, Loss G: 2.3891\nEpoch [1/10] Step [0/1583] Loss D: 0.5011, Loss G: 3.5703\nEpoch [1/10] Step [100/1583] Loss D: 0.4375, Loss G: 3.2219\nEpoch [1/10] Step [200/1583] Loss D: 0.9647, Loss G: 2.2166\nEpoch [1/10] Step [300/1583] Loss D: 0.3633, Loss G: 4.3474\nEpoch [1/10] Step [400/1583] Loss D: 0.6680, Loss G: 3.8134\nEpoch [1/10] Step [500/1583] Loss D: 0.3417, Loss G: 4.2077\nEpoch [1/10] Step [600/1583] Loss D: 0.5080, Loss G: 4.5093\nEpoch [1/10] Step [700/1583] Loss D: 0.4517, Loss G: 3.0497\nEpoch [1/10] Step [800/1583] Loss D: 0.6528, Loss G: 2.2166\nEpoch [1/10] Step [900/1583] Loss D: 2.2983, Loss G: 0.2394\nEpoch [1/10] Step [1000/1583] Loss D: 0.9477, Loss G: 0.9626\nEpoch [1/10] Step [1100/1583] Loss D: 1.2442, Loss G: 6.4661\nEpoch [1/10] Step [1200/1583] Loss D: 0.3537, Loss G: 3.2727\nEpoch [1/10] Step [1300/1583] Loss D: 0.5905, Loss G: 3.4691\nEpoch [1/10] Step [1400/1583] Loss D: 0.4432, Loss G: 3.8783\nEpoch [1/10] Step [1500/1583] Loss D: 0.5725, Loss G: 2.9408\nEpoch [2/10] Step [0/1583] Loss D: 0.4971, Loss G: 2.4472\nEpoch [2/10] Step [100/1583] Loss D: 1.0208, Loss G: 1.9533\nEpoch [2/10] Step [200/1583] Loss D: 1.0788, Loss G: 4.6487\nEpoch [2/10] Step [300/1583] Loss D: 0.8306, Loss G: 1.9327\nEpoch [2/10] Step [400/1583] Loss D: 1.2299, Loss G: 4.2000\nEpoch [2/10] Step [500/1583] Loss D: 0.5439, Loss G: 3.1361\nEpoch [2/10] Step [600/1583] Loss D: 0.5520, Loss G: 2.6728\nEpoch [2/10] Step [700/1583] Loss D: 1.1174, Loss G: 1.5501\nEpoch [2/10] Step [800/1583] Loss D: 0.5791, Loss G: 2.3222\nEpoch [2/10] Step [900/1583] Loss D: 0.4775, Loss G: 2.5183\nEpoch [2/10] Step [1000/1583] Loss D: 0.7872, Loss G: 1.0836\nEpoch [2/10] Step [1100/1583] Loss D: 0.6386, Loss G: 2.1169\nEpoch [2/10] Step [1200/1583] Loss D: 0.4622, Loss G: 2.2884\nEpoch [2/10] Step [1300/1583] Loss D: 0.6012, Loss G: 2.7027\nEpoch [2/10] Step [1400/1583] Loss D: 0.5697, Loss G: 3.4840\nEpoch [2/10] Step [1500/1583] Loss D: 0.5399, Loss G: 2.4102\nEpoch [3/10] Step [0/1583] Loss D: 0.6105, Loss G: 2.7406\nEpoch [3/10] Step [100/1583] Loss D: 0.5668, Loss G: 2.4532\nEpoch [3/10] Step [200/1583] Loss D: 0.5837, Loss G: 1.8397\nEpoch [3/10] Step [300/1583] Loss D: 0.7270, Loss G: 2.6765\nEpoch [3/10] Step [400/1583] Loss D: 0.7416, Loss G: 1.8471\nEpoch [3/10] Step [500/1583] Loss D: 0.6607, Loss G: 3.7370\nEpoch [3/10] Step [600/1583] Loss D: 1.2628, Loss G: 4.4705\nEpoch [3/10] Step [700/1583] Loss D: 0.6027, Loss G: 3.3591\nEpoch [3/10] Step [800/1583] Loss D: 0.4965, Loss G: 2.9883\nEpoch [3/10] Step [900/1583] Loss D: 0.5588, Loss G: 3.0339\nEpoch [3/10] Step [1000/1583] Loss D: 0.4556, Loss G: 2.7992\nEpoch [3/10] Step [1100/1583] Loss D: 0.9042, Loss G: 4.6342\nEpoch [3/10] Step [1200/1583] Loss D: 1.4170, Loss G: 0.7253\nEpoch [3/10] Step [1300/1583] Loss D: 0.6232, Loss G: 4.1539\nEpoch [3/10] Step [1400/1583] Loss D: 0.6809, Loss G: 1.9695\nEpoch [3/10] Step [1500/1583] Loss D: 0.6675, Loss G: 1.6792\nEpoch [4/10] Step [0/1583] Loss D: 0.6200, Loss G: 4.0826\nEpoch [4/10] Step [100/1583] Loss D: 0.5830, Loss G: 1.9854\nEpoch [4/10] Step [200/1583] Loss D: 0.5779, Loss G: 1.9170\nEpoch [4/10] Step [300/1583] Loss D: 0.6057, Loss G: 3.4122\nEpoch [4/10] Step [400/1583] Loss D: 0.5767, Loss G: 3.7090\nEpoch [4/10] Step [500/1583] Loss D: 0.6187, Loss G: 3.6351\nEpoch [4/10] Step [600/1583] Loss D: 0.7089, Loss G: 1.3518\nEpoch [4/10] Step [700/1583] Loss D: 0.8525, Loss G: 1.1051\nEpoch [4/10] Step [800/1583] Loss D: 0.5834, Loss G: 2.5334\nEpoch [4/10] Step [900/1583] Loss D: 0.5318, Loss G: 2.8671\nEpoch [4/10] Step [1000/1583] Loss D: 0.5401, Loss G: 3.2917\nEpoch [4/10] Step [1100/1583] Loss D: 0.5477, Loss G: 4.0065\nEpoch [4/10] Step [1200/1583] Loss D: 0.7950, Loss G: 3.3816\nEpoch [4/10] Step [1300/1583] Loss D: 0.7099, Loss G: 1.5950\nEpoch [4/10] Step [1400/1583] Loss D: 3.2709, Loss G: 3.9117\nEpoch [4/10] Step [1500/1583] Loss D: 0.5600, Loss G: 3.7037\nEpoch [5/10] Step [0/1583] Loss D: 2.2140, Loss G: 6.3618\nEpoch [5/10] Step [100/1583] Loss D: 0.4818, Loss G: 3.0008\nEpoch [5/10] Step [200/1583] Loss D: 0.5879, Loss G: 1.7766\nEpoch [5/10] Step [300/1583] Loss D: 0.6448, Loss G: 2.2742\nEpoch [5/10] Step [400/1583] Loss D: 0.8277, Loss G: 0.8698\nEpoch [5/10] Step [500/1583] Loss D: 0.6393, Loss G: 3.1644\nEpoch [5/10] Step [600/1583] Loss D: 0.5410, Loss G: 2.7082\nEpoch [5/10] Step [700/1583] Loss D: 1.1165, Loss G: 0.7603\nEpoch [5/10] Step [800/1583] Loss D: 0.8465, Loss G: 3.1364\nEpoch [5/10] Step [900/1583] Loss D: 0.8435, Loss G: 1.1111\nEpoch [5/10] Step [1000/1583] Loss D: 0.6256, Loss G: 1.9220\nEpoch [5/10] Step [1100/1583] Loss D: 0.7357, Loss G: 1.1978\nEpoch [5/10] Step [1200/1583] Loss D: 0.4562, Loss G: 2.0659\nEpoch [5/10] Step [1300/1583] Loss D: 0.6686, Loss G: 1.4482\nEpoch [5/10] Step [1400/1583] Loss D: 0.5012, Loss G: 2.2968\nEpoch [5/10] Step [1500/1583] Loss D: 0.5865, Loss G: 3.1459\nEpoch [6/10] Step [0/1583] Loss D: 0.4155, Loss G: 2.4374\nEpoch [6/10] Step [100/1583] Loss D: 1.1958, Loss G: 5.3035\nEpoch [6/10] Step [200/1583] Loss D: 0.7700, Loss G: 3.6513\nEpoch [6/10] Step [300/1583] Loss D: 0.5258, Loss G: 2.1608\nEpoch [6/10] Step [400/1583] Loss D: 0.3903, Loss G: 3.0923\nEpoch [6/10] Step [500/1583] Loss D: 0.4720, Loss G: 3.0332\nEpoch [6/10] Step [600/1583] Loss D: 1.2695, Loss G: 5.9255\nEpoch [6/10] Step [700/1583] Loss D: 0.4923, Loss G: 2.5509\nEpoch [6/10] Step [800/1583] Loss D: 0.4578, Loss G: 1.8862\nEpoch [6/10] Step [900/1583] Loss D: 0.7110, Loss G: 3.6197\nEpoch [6/10] Step [1000/1583] Loss D: 0.5744, Loss G: 2.1085\nEpoch [6/10] Step [1100/1583] Loss D: 0.3938, Loss G: 2.9466\nEpoch [6/10] Step [1200/1583] Loss D: 1.2346, Loss G: 0.9279\nEpoch [6/10] Step [1300/1583] Loss D: 0.7010, Loss G: 4.7662\nEpoch [6/10] Step [1400/1583] Loss D: 0.5114, Loss G: 2.1596\nEpoch [6/10] Step [1500/1583] Loss D: 0.3650, Loss G: 2.7020\nEpoch [7/10] Step [0/1583] Loss D: 1.9734, Loss G: 5.4035\nEpoch [7/10] Step [100/1583] Loss D: 0.5767, Loss G: 2.2773\nEpoch [7/10] Step [200/1583] Loss D: 0.5739, Loss G: 3.4935\nEpoch [7/10] Step [300/1583] Loss D: 0.6254, Loss G: 3.5513\nEpoch [7/10] Step [400/1583] Loss D: 0.6636, Loss G: 2.5720\nEpoch [7/10] Step [500/1583] Loss D: 0.4621, Loss G: 4.2787\nEpoch [7/10] Step [600/1583] Loss D: 0.5378, Loss G: 3.8528\nEpoch [7/10] Step [700/1583] Loss D: 0.6914, Loss G: 4.3435\nEpoch [7/10] Step [800/1583] Loss D: 0.4442, Loss G: 2.4459\nEpoch [7/10] Step [900/1583] Loss D: 0.3554, Loss G: 2.0680\nEpoch [7/10] Step [1000/1583] Loss D: 0.5504, Loss G: 3.8710\nEpoch [7/10] Step [1100/1583] Loss D: 0.5834, Loss G: 1.6110\nEpoch [7/10] Step [1200/1583] Loss D: 0.5166, Loss G: 2.5442\nEpoch [7/10] Step [1300/1583] Loss D: 0.3877, Loss G: 1.9447\nEpoch [7/10] Step [1400/1583] Loss D: 0.3094, Loss G: 3.4453\nEpoch [7/10] Step [1500/1583] Loss D: 0.5938, Loss G: 2.4975\nEpoch [8/10] Step [0/1583] Loss D: 0.8763, Loss G: 0.8300\nEpoch [8/10] Step [100/1583] Loss D: 0.6111, Loss G: 4.1446\nEpoch [8/10] Step [200/1583] Loss D: 0.3788, Loss G: 1.7836\nEpoch [8/10] Step [300/1583] Loss D: 0.5919, Loss G: 2.4850\nEpoch [8/10] Step [400/1583] Loss D: 0.5689, Loss G: 1.8359\nEpoch [8/10] Step [500/1583] Loss D: 0.8775, Loss G: 5.3221\nEpoch [8/10] Step [600/1583] Loss D: 1.3150, Loss G: 0.1007\nEpoch [8/10] Step [700/1583] Loss D: 0.5313, Loss G: 2.0245\nEpoch [8/10] Step [800/1583] Loss D: 0.4040, Loss G: 1.7300\nEpoch [8/10] Step [900/1583] Loss D: 0.4339, Loss G: 2.8696\nEpoch [8/10] Step [1000/1583] Loss D: 0.5777, Loss G: 1.2253\nEpoch [8/10] Step [1100/1583] Loss D: 1.2874, Loss G: 1.1418\nEpoch [8/10] Step [1200/1583] Loss D: 0.3219, Loss G: 2.9180\nEpoch [8/10] Step [1300/1583] Loss D: 0.6289, Loss G: 5.2867\nEpoch [8/10] Step [1400/1583] Loss D: 0.4066, Loss G: 3.3398\nEpoch [8/10] Step [1500/1583] Loss D: 0.4950, Loss G: 2.1835\nEpoch [9/10] Step [0/1583] Loss D: 0.4493, Loss G: 2.4520\nEpoch [9/10] Step [100/1583] Loss D: 1.3821, Loss G: 1.2654\nEpoch [9/10] Step [200/1583] Loss D: 0.2723, Loss G: 3.1262\nEpoch [9/10] Step [300/1583] Loss D: 0.3269, Loss G: 3.6450\nEpoch [9/10] Step [400/1583] Loss D: 1.9906, Loss G: 5.4202\nEpoch [9/10] Step [500/1583] Loss D: 0.4062, Loss G: 2.3515\nEpoch [9/10] Step [600/1583] Loss D: 0.3197, Loss G: 2.5189\nEpoch [9/10] Step [700/1583] Loss D: 0.2430, Loss G: 3.0494\nEpoch [9/10] Step [800/1583] Loss D: 0.2143, Loss G: 3.0152\nEpoch [9/10] Step [900/1583] Loss D: 0.5416, Loss G: 2.7621\nEpoch [9/10] Step [1000/1583] Loss D: 0.3728, Loss G: 2.7608\nEpoch [9/10] Step [1100/1583] Loss D: 0.1647, Loss G: 3.6969\nEpoch [9/10] Step [1200/1583] Loss D: 0.3848, Loss G: 2.7055\nEpoch [9/10] Step [1300/1583] Loss D: 0.4486, Loss G: 4.8470\nEpoch [9/10] Step [1400/1583] Loss D: 0.3832, Loss G: 4.6159\nEpoch [9/10] Step [1500/1583] Loss D: 2.3123, Loss G: 0.3494\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}